{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishvesh-Bhardwaj/Machine-Learning-Core-Concepts/blob/main/RA2011026010109_MLCC_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RA2011026010109 - VISHVESH BHARDWAJ - Principal component analysis**"
      ],
      "metadata": {
        "id": "M0OMMJSeifyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the famous Iris dataset, which contains measurements of sepal length, sepal width, petal length, and petal width for **150 iris flowers, with 50 observations* for each of the three species:** ***Iris setosa, Iris versicolor, and Iris virginica.**\n",
        "\n",
        "**First, let's load the dataset and split it into training and testing sets:**"
      ],
      "metadata": {
        "id": "UrGs-HQbTA26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OwktmLbqS21P"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, let's create a logistic regression model and train it on the original dataset:**"
      ],
      "metadata": {
        "id": "RyMuydkZTd8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy on training set:\", lr.score(X_train, y_train))\n",
        "print(\"Accuracy on testing set:\", lr.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahfaxAyxTcPs",
        "outputId": "941077e5-7b5b-423e-a957-d2a738a8bee4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.975\n",
            "Accuracy on testing set: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us an accuracy of around **97.5%** on the training set and **100%** on the testing set, which indicates that the ***model is overfitting.***\n",
        "\n",
        "Now, **let's apply PCA to the dataset to reduce its dimensionality.**\n",
        "We'll keep only the first two principal components, which should capture most of the variance in the data:"
      ],
      "metadata": {
        "id": "DsJd4O9YTrsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "a0cVRTBFTnSN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finally, let's train the logistic regression model on the reduced dataset:**"
      ],
      "metadata": {
        "id": "c4kf-2R7UF0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pca = LogisticRegression()\n",
        "lr_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "print(\"Accuracy on training set (after PCA):\", lr_pca.score(X_train_pca, y_train))\n",
        "print(\"Accuracy on testing set (after PCA):\", lr_pca.score(X_test_pca, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOHWwOdLUGuZ",
        "outputId": "203b0f95-dd5c-4045-d894-907d58466101"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set (after PCA): 0.9583333333333334\n",
            "Accuracy on testing set (after PCA): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us an accuracy of around **95% on the training set and 100% on the testing set**, which indicates that the model is still performing well but with ***less overfitting.***\n",
        "\n",
        "Overall, **we can see that applying PCA to the Iris dataset led to a reduction in the dimensionality of the data while maintaining high accuracy on the testing set.**\n",
        "\n",
        "*However, the reduction in dimensionality also led to a slight decrease in accuracy on the training set, which is to be expected when we discard some of the information in the original dataset.*"
      ],
      "metadata": {
        "id": "wWdhaS_HUL0r"
      }
    }
  ]
}